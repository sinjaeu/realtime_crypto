===============================================================================
                    암호화폐 실시간 데이터 처리 프로젝트 구현 계획서
                              (Kafka + Airflow Architecture)
===============================================================================

📋 프로젝트 개요
---------------
- 목표: 실시간 암호화폐 데이터 수집, 처리, 분석 시스템 구축
- 아키텍처: Kafka (실시간 스트리밍) + Airflow (배치 처리)
- 기술 스택: Docker, Kafka, Airflow, Python, PostgreSQL, Redis
- 예상 소요시간: 15-22시간 (1-2주)

===============================================================================
                                 구현 단계별 계획
===============================================================================

🏗️ PHASE 1: 인프라 구성 (Kafka 환경 구축)
-------------------------------------------
목표: Docker Compose에 Kafka 추가 및 기본 설정

작업 내용:
1. docker-compose.yaml에 Kafka, Zookeeper 추가
2. Kafka UI 도구 추가 (AKHQ 또는 Kafka-UI)
3. 필요한 환경변수 .env에 추가
4. Kafka 네트워크 설정

테스트 기준:
- docker-compose up -d 성공
- Kafka UI 접속 확인 (http://localhost:9000)
- Kafka 클러스터 상태 확인

체크리스트:
□ Kafka 컨테이너 정상 시작
□ Zookeeper 연결 확인
□ Kafka UI 접속 가능
□ 기본 토픽 생성 확인

예상 소요시간: 1-2시간
완료 기준: Kafka 환경 구축 완료, 기본 인프라 준비 완료

===============================================================================

📡 PHASE 2: 기본 Producer 구현
-----------------------------
목표: 실시간 데이터 수집을 위한 Kafka Producer 구현

작업 내용:
1. kafka_services/producers/ 폴더 생성
2. crypto_producer.py 기본 구현
3. Kafka 토픽 생성 (crypto-prices)
4. 기본 에러 처리 및 로깅

테스트 기준:
- Producer 단독 실행 성공
- Kafka UI에서 메시지 확인
- 데이터 형식 검증

체크리스트:
□ Producer 코드 작성 완료
□ Binance API 연결 성공
□ Kafka 토픽에 메시지 전송 확인
□ 에러 처리 구현

예상 소요시간: 2-3시간
완료 기준: 실시간 데이터 수집 가능, Kafka 메시지 확인 가능

===============================================================================

🎯 PHASE 3: 기본 Consumer 구현
------------------------------
목표: Kafka에서 데이터를 소비하는 기본 Consumer 구현

작업 내용:
1. kafka_services/consumers/ 폴더 생성
2. crypto_consumer.py 기본 구현
3. 실시간 데이터 처리 로직
4. Redis 캐싱 연동

테스트 기준:
- Consumer 단독 실행 성공
- Producer → Consumer 데이터 전달 확인
- 실시간 데이터 확인

체크리스트:
□ Consumer 코드 작성 완료
□ Kafka에서 메시지 수신 확인
□ 실시간 데이터 처리 확인
□ Redis 캐싱 동작 확인

예상 소요시간: 2-3시간
완료 기준: 실시간 데이터 처리 가능, 기본 스트리밍 파이프라인 완성

===============================================================================

🔄 PHASE 4: Airflow DAG 수정
-----------------------------
목표: 기존 DAG를 Kafka 기반 배치 처리로 변경

작업 내용:
1. 기존 crypo_data.py 백업
2. crypto_batch_processing.py 새로 생성
3. Kafka Consumer를 통한 배치 처리
4. 데이터 저장 로직 구현

테스트 기준:
- Airflow DAG 실행 성공
- Kafka → Airflow → Database 플로우 확인
- 데이터 품질 검증

체크리스트:
□ 새로운 Airflow DAG 생성
□ Kafka Consumer 통합
□ 배치 처리 로직 구현
□ 데이터베이스 저장 확인

예상 소요시간: 3-4시간
완료 기준: 배치 처리 시스템 완성, 전체 데이터 플로우 구축 완료

===============================================================================

📚 PHASE 5: 공통 라이브러리 구축
--------------------------------
목표: 중복 코드 제거 및 공통 기능 모듈화

작업 내용:
1. shared/ 폴더 생성
2. database.py - DB 연결 및 CRUD
3. utils.py - 데이터 변환 함수
4. models.py - 데이터 구조 정의
5. config.py - 설정 관리

테스트 기준:
- 모든 컴포넌트에서 공통 라이브러리 사용
- 코드 중복 제거 확인
- 설정 통합 관리 확인

체크리스트:
□ 공통 라이브러리 모듈 생성
□ 중복 코드 제거
□ 설정 통합 관리
□ 코드 재사용성 향상

예상 소요시간: 2-3시간
완료 기준: 코드 품질 향상, 유지보수성 증대

===============================================================================

⚙️ PHASE 6: 운영 스크립트 및 모니터링
-------------------------------------
목표: 시스템 운영 및 모니터링 자동화

작업 내용:
1. scripts/ 폴더 생성
2. start_producers.sh - Producer 일괄 시작
3. health_check.py - 시스템 상태 확인
4. 로그 시스템 구성
5. 알림 시스템 구성

테스트 기준:
- 스크립트 실행 성공
- 헬스체크 기능 확인
- 로그 수집 확인
- 알림 기능 테스트

체크리스트:
□ 운영 스크립트 작성
□ 모니터링 시스템 구축
□ 로그 시스템 구성
□ 알림 시스템 구현

예상 소요시간: 2-3시간
완료 기준: 운영 자동화 완성, 모니터링 시스템 구축

===============================================================================

🧪 PHASE 7: 테스트 및 최적화
-----------------------------
목표: 시스템 안정성 확보 및 성능 최적화

작업 내용:
1. tests/ 폴더 생성
2. 단위 테스트 작성
3. 통합 테스트 작성
4. 성능 최적화
5. 문서화 (README.md)

테스트 기준:
- 모든 테스트 통과
- 부하 테스트 수행
- 장애 복구 테스트
- 문서 완성도 확인

체크리스트:
□ 테스트 코드 작성
□ 성능 최적화 완료
□ 문서화 완료
□ 프로덕션 준비 완료

예상 소요시간: 3-4시간
완료 기준: 프로덕션 레디 시스템 완성, 전체 프로젝트 완료

===============================================================================
                                최종 폴더 구조
===============================================================================

realtime_data/
├── kafka_services/              # Kafka 관련 서비스들
│   ├── producers/               # 데이터 생산자들
│   │   ├── __init__.py
│   │   ├── crypto_producer.py         # 암호화폐 데이터 수집
│   │   ├── news_producer.py           # 뉴스 데이터 수집
│   │   └── config.py                 # Producer 설정
│   ├── consumers/               # 데이터 소비자들
│   │   ├── __init__.py
│   │   ├── crypto_consumer.py         # 실시간 처리용
│   │   └── alert_consumer.py          # 알림 처리용
│   └── topics/                  # 토픽 설정
│       ├── __init__.py
│       └── topic_config.py
├── dags/                        # Airflow DAG들
│   ├── crypto_batch_processing.py      # 배치 처리
│   ├── crypto_analysis.py              # 데이터 분석
│   ├── crypto_reports.py               # 리포트 생성
│   └── data_quality_check.py           # 데이터 품질 검사
├── shared/                      # 공통 라이브러리
│   ├── __init__.py
│   ├── database.py              # DB 연결
│   ├── utils.py                 # 유틸리티 함수
│   ├── models.py                # 데이터 모델
│   └── config.py                # 공통 설정
├── scripts/                     # 운영 스크립트
│   ├── start_producers.sh       # Producer 시작
│   ├── stop_producers.sh        # Producer 중지
│   ├── setup_kafka.sh           # Kafka 초기 설정
│   └── health_check.py          # 헬스체크
├── config/                      # 설정 파일들
│   ├── kafka_config.yaml        # Kafka 설정
│   ├── airflow_config.yaml      # Airflow 설정
│   └── database_config.yaml     # DB 설정
├── tests/                       # 테스트 코드
│   ├── test_producers.py
│   ├── test_consumers.py
│   └── test_dags.py
├── logs/                        # 로그 파일들
│   ├── airflow/
│   ├── kafka/
│   └── producers/
├── plugins/                     # Airflow 플러그인
├── docker/                      # Docker 관련 파일들
│   ├── kafka.dockerfile
│   ├── airflow.dockerfile
│   └── producer.dockerfile
├── docker-compose.yaml          # Docker Compose
├── .env                         # 환경 변수
├── .gitignore                   # Git 무시 파일
└── README.md                    # 프로젝트 설명

===============================================================================
                                데이터 플로우
===============================================================================

실시간 스트리밍 레이어 (Kafka):
API → Kafka Producer → Kafka Topics → Kafka Consumer → 실시간 처리

배치 처리 레이어 (Airflow):
Kafka Topics → Airflow DAG → 데이터 정제 → 데이터베이스 저장

전체 플로우:
1. kafka_services/producers: 24/7 실시간 API 호출
2. Kafka Topics: 데이터 버퍼링 및 분산 처리
3. kafka_services/consumers: 실시간 알림 및 캐싱
4. Airflow DAGs: 주기적 배치 처리 및 분석
5. Database: 최종 데이터 저장 및 조회

===============================================================================
                                중요 참고사항
===============================================================================

진행 방식:
1. 각 Phase 완료 후 반드시 테스트 수행
2. 문제 발생 시 다음 단계 진행 금지
3. 각 단계별 Git 커밋 권장
4. Phase 4 완료 후 중간 검토 추천

성공 기준:
- 모든 체크리스트 완료
- 테스트 기준 통과
- 시스템 안정성 확보
- 문서화 완료

===============================================================================
                                    END
=============================================================================== 